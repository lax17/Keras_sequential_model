{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_sequental_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FAjTTLJyl_3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "101a5691-9ba1-45f8-a668-51dad4ef9585"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTPcrx2pNZfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==1.14.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFpL9YBqNfZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CABVaIWUiipH",
        "colab_type": "text"
      },
      "source": [
        "**Importing all necessary libraries:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZTAIo6bM4ff",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f930d922-f6a9-41e6-ac54-582b4aa88f6a"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator \n",
        "from keras.models import Sequential \n",
        "from keras.layers import Conv2D, MaxPooling2D \n",
        "from keras.layers import Activation, Dropout, Flatten, Dense \n",
        "from keras import backend as K "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqyecWa3Nlmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Every image in the dataset is of the size 224*224.\"\"\"\n",
        "\"\"\"the train_data_dir is the train dataset directory. \n",
        "  validation_data_dir is the directory for validation data.\n",
        "  nb_train_samples is the total number train samples. \n",
        "  nb_validation_samples is the total number of validation samples\"\"\"\n",
        "\n",
        "import os\n",
        "train_data_dir = \"/content/drive/My Drive/v_data/train\"\n",
        "validation_data_dir ='/content/drive/My Drive/v_data/test'\n",
        "nb_train_samples =400 \n",
        "nb_validation_samples = 100\n",
        "epochs = 10\n",
        "batch_size = 16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbaO7sQxiEPg",
        "colab_type": "text"
      },
      "source": [
        "**Checking format of Image:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo19NCYtN5y5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " img_width= img_height=224\n",
        "\n",
        " \"\"\"This part is to check the data format i.e the RGB channel is coming first or last so,\n",
        "  whatever it may be, model will check first and then input shape will be feeded accordingly.\"\"\"\n",
        "\n",
        "if K.image_data_format() == 'channels_first': \n",
        "    input_shape = (3, img_width, img_height) \n",
        "else: \n",
        "    input_shape = (img_width, img_height, 3) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6ZB2xdHlJRd",
        "colab_type": "text"
      },
      "source": [
        "**Layer Definition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbK4AwBpO5lB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Conv2D is the layer to convolve the image into multiple images\n",
        "Activation is the activation function.\n",
        "MaxPooling2D is used to max pool the value from the given size matrix and same is used for the next 2 layers. then, Flatten is used to flatten the dimensions of the image obtained after convolving it.\n",
        "Dense is used to make this a fully connected model and is the hidden layer.\n",
        "Dropout is used to avoid overfitting on the dataset.\n",
        "Dense is the output layer contains only one neuron which decide to which category image belongs.\n",
        "\"\"\"\n",
        "\n",
        "model = Sequential() \n",
        "model.add(Conv2D(32, (2, 2), input_shape=input_shape)) \n",
        "model.add(Activation('relu')) \n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
        "  \n",
        "model.add(Conv2D(32, (2, 2))) \n",
        "model.add(Activation('relu')) \n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
        "  \n",
        "model.add(Conv2D(64, (2, 2))) \n",
        "model.add(Activation('relu')) \n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
        "  \n",
        "model.add(Flatten()) \n",
        "model.add(Dense(64)) \n",
        "model.add(Activation('relu')) \n",
        "model.add(Dropout(rate=0.5)) \n",
        "model.add(Dense(1)) \n",
        "model.add(Activation('sigmoid')) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjAe9EqnlBwL",
        "colab_type": "text"
      },
      "source": [
        "**Compile Function:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZffnMUoJRm4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Compile function is used here that involve use of loss, optimizers and metrics.\n",
        "here loss function used is binary_crossentropy, optimizer used is rmsprop.\n",
        "\"\"\"\n",
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='rmsprop', \n",
        "              metrics=['accuracy']) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePwWilP9lWeD",
        "colab_type": "text"
      },
      "source": [
        "**Using DataGenerator:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44T9uoF0Rt6O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "b8522fca-646f-43b1-dce9-4558e8de793a"
      },
      "source": [
        "\"\"\"\n",
        "ImageDataGenerator : that rescales the image, applies shear in some range, zooms the image and does horizontal flipping with the image. This ImageDataGenerator includes all possible orientation of the image.\n",
        "train_datagen.flow_from_directory : is the function that is used to prepare data from the train_dataset directory Target_size specifies the target size of the image.\n",
        "test_datagen.flow_from_directory : is used to prepare test data for the model and all is similar as above.\n",
        "fit_generator : is used to fit the data into the model made above, other factors used are steps_per_epochs tells us about the number of times the model will execute for the training data.\n",
        "epochs : tells us the number of times model will be trained in forward and backward pass.\n",
        "validation_data : is used to feed the validation/test data into the model.\n",
        "validation_steps : denotes the number of validation/test samples.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator( \n",
        "                rescale = 1. / 255, \n",
        "                 shear_range = 0.2, \n",
        "                  zoom_range = 0.2, \n",
        "            horizontal_flip = True) \n",
        "  \n",
        "test_datagen = ImageDataGenerator(rescale = 1. / 255) \n",
        "  \n",
        "train_generator = train_datagen.flow_from_directory(train_data_dir, \n",
        "                              target_size =(img_width, img_height), \n",
        "                     batch_size = 16, class_mode ='binary') \n",
        "  \n",
        "validation_generator = test_datagen.flow_from_directory( \n",
        "                                    validation_data_dir, \n",
        "                   target_size =(img_width, img_height), \n",
        "          batch_size = 16, class_mode ='binary') \n",
        "  \n",
        "model.fit_generator(train_generator, \n",
        "    steps_per_epoch = nb_train_samples // batch_size, \n",
        "    epochs = epochs, validation_data = validation_generator, \n",
        "    validation_steps = nb_validation_samples // batch_size)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400 images belonging to 2 classes.\n",
            "Found 100 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "25/25 [==============================] - 185s 7s/step - loss: 0.7352 - acc: 0.6200 - val_loss: 0.3822 - val_acc: 0.8229\n",
            "Epoch 2/10\n",
            "25/25 [==============================] - 22s 898ms/step - loss: 0.5436 - acc: 0.7375 - val_loss: 0.3929 - val_acc: 0.7976\n",
            "Epoch 3/10\n",
            "25/25 [==============================] - 22s 897ms/step - loss: 0.4762 - acc: 0.8100 - val_loss: 0.3124 - val_acc: 0.9524\n",
            "Epoch 4/10\n",
            "25/25 [==============================] - 23s 911ms/step - loss: 0.4199 - acc: 0.8225 - val_loss: 0.4214 - val_acc: 0.8452\n",
            "Epoch 5/10\n",
            "25/25 [==============================] - 23s 915ms/step - loss: 0.4101 - acc: 0.8225 - val_loss: 0.2818 - val_acc: 0.8810\n",
            "Epoch 6/10\n",
            "25/25 [==============================] - 23s 905ms/step - loss: 0.3866 - acc: 0.8425 - val_loss: 0.2512 - val_acc: 0.9048\n",
            "Epoch 7/10\n",
            "25/25 [==============================] - 22s 885ms/step - loss: 0.3121 - acc: 0.8675 - val_loss: 0.4637 - val_acc: 0.7738\n",
            "Epoch 8/10\n",
            "25/25 [==============================] - 23s 912ms/step - loss: 0.2733 - acc: 0.8875 - val_loss: 0.4571 - val_acc: 0.8229\n",
            "Epoch 9/10\n",
            "25/25 [==============================] - 22s 898ms/step - loss: 0.3014 - acc: 0.8950 - val_loss: 0.3059 - val_acc: 0.8690\n",
            "Epoch 10/10\n",
            "25/25 [==============================] - 23s 904ms/step - loss: 0.2558 - acc: 0.8925 - val_loss: 0.3249 - val_acc: 0.8929\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4e0b683a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSuEFMOAmGfo",
        "colab_type": "text"
      },
      "source": [
        "**At last we can also save the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFO-cJ0BcEDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('model_saved.h5') "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}